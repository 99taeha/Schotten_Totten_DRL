{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vcd9X9gTiZY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from collections import deque\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKonQSRKTiZl"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_nodes, hidden_layer_size, output_nodes):\n",
        "        super(Actor, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_nodes, input_nodes))\n",
        "        self.layers.append(nn.SELU())\n",
        "        for size in hidden_layer_size:\n",
        "            self.layers.append(nn.Linear(input_nodes, size))\n",
        "            self.layers.append(nn.SELU())\n",
        "            input_nodes = size\n",
        "        self.layers.append(nn.Linear(input_nodes, output_nodes))\n",
        "        self.layers.append(nn.Softmax(dim=-1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpDueP9xCz88"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_nodes, hidden_layer_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(nn.Linear(input_nodes, input_nodes))\n",
        "        self.layers.append(nn.SELU())\n",
        "        for size in hidden_layer_size:\n",
        "            self.layers.append(nn.Linear(input_nodes, size))\n",
        "            self.layers.append(nn.SELU())\n",
        "            input_nodes = size\n",
        "        self.layers.append(nn.Linear(input_nodes, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfiuGhwo3Jdg"
      },
      "outputs": [],
      "source": [
        "def train(states, actions, rewards, next_states, dones, actor, critic, actor_optimizer, critic_optimizer):\n",
        "    # Compute value targets\n",
        "\n",
        "    max_norm = 1\n",
        "\n",
        "    next_state_values = critic(next_states).squeeze(-1)\n",
        "\n",
        "    # Convert dones to tensor if they're not\n",
        "    if not isinstance(dones, torch.Tensor):\n",
        "        dones = torch.tensor(dones).to(states.device)\n",
        "\n",
        "    # Convert dones to float if they're boolean\n",
        "    if dones.dtype == torch.bool:\n",
        "        dones = dones.float()\n",
        "\n",
        "    targets = rewards + (1 - dones) * gamma * next_state_values\n",
        "\n",
        "    # Update critic\n",
        "    critic_values = critic(states)\n",
        "    critic_loss = F.mse_loss(critic_values, targets.detach())\n",
        "\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    torch_utils.clip_grad_norm_(critic.parameters(), max_norm)\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    # Update actor\n",
        "    log_probs = torch.log(actor(states))\n",
        "\n",
        "    #print(log_probs[actions])\n",
        "\n",
        "    actor_loss = -(critic(states) * log_probs[actions]).mean()\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    torch_utils.clip_grad_norm_(actor.parameters(), max_norm)\n",
        "    actor_optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiAwMOvRTiZn"
      },
      "outputs": [],
      "source": [
        "# Implement replay buffer\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, maxlength):\n",
        "        self.buffer = deque()\n",
        "        self.number = 0\n",
        "        self.maxlength = maxlength\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "        self.number += 1\n",
        "        if(self.number > self.maxlength):\n",
        "            self.pop()\n",
        "\n",
        "    def pop(self):\n",
        "        while self.number > self.maxlength:\n",
        "            self.buffer.popleft()\n",
        "            self.number -= 1\n",
        "\n",
        "    def sample(self, batchsize):\n",
        "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
        "        return [self.buffer[idx] for idx in inds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5gfq0NwcsnX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from Agent import Agent\n",
        "from Game import Game\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as torch_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77FshpPhc3WT"
      },
      "outputs": [],
      "source": [
        "lr = 1e-4 # learning rate for gradient update\n",
        "batchsize = 64  # batchsize for buffer sampling\n",
        "maxlength = 1000  # max number of tuples held by buffer\n",
        "tau = 1000  # time steps for target update\n",
        "episodes = 10000  # number of episodes to run\n",
        "initialize = 1000  # initial time steps before start updating\n",
        "epsilon = .1  # constant for exploration\n",
        "gamma = .99 # discount\n",
        "hidden_dims=[128, 512, 256, 64] # hidden dimensions\n",
        "\n",
        "obssize = 120\n",
        "actsize = 54"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTPmiCU-xg4x"
      },
      "outputs": [],
      "source": [
        "# initialize networks\n",
        "actor = Actor(obssize, hidden_dims, actsize)\n",
        "critic = Critic(obssize, hidden_dims)\n",
        "actor_optimizer = torch.optim.Adam(actor.parameters(), lr=lr)\n",
        "critic_optimizer = torch.optim.Adam(critic.parameters(), lr=lr)\n",
        "\n",
        "def lecun_init(m):\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight, gain=1.0)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "actor.apply(lecun_init)\n",
        "critic.apply(lecun_init)\n",
        "\n",
        "'''\n",
        "if next(Qprincipal.qfunction.parameters()).is_cuda:\n",
        "    print(\"Model is running on GPU\")\n",
        "else:\n",
        "    print(\"Model is running on CPU\")\n",
        "'''\n",
        "\n",
        "game = Game(9)\n",
        "buffer = ReplayBuffer(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jq-7TPhaETAD",
        "outputId": "225733c9-087f-402a-b04e-3ea45b556123"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "print(torch.cuda.device_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y-i55SS-1-g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "wins = []\n",
        "unif_agent = Agent()\n",
        "maxWinRate = 0\n",
        "baseline = 1\n",
        "actor = actor.to(device)\n",
        "critic = critic.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsAgsWTf3qmP",
        "outputId": "e960fc5f-1727-42e6-d5d6-749e1ab604e7"
      },
      "outputs": [],
      "source": [
        "rrecord = []\n",
        "totalstep = 0\n",
        "\n",
        "for ite in range(episodes):\n",
        "    obs = torch.from_numpy(game.reset(display=0)).float().to(device)\n",
        "    done = False\n",
        "    rsum = 0\n",
        "\n",
        "    while not done:\n",
        "        totalstep += 1\n",
        "        if np.random.rand() < max(epsilon, 1-(1-epsilon)/episodes/0.6*ite):\n",
        "            action = random.choice(range(actsize))\n",
        "        else:\n",
        "            action_probs = actor(obs.to(device))\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        obs_tmp, _, done = game.step(action)\n",
        "        obs_tmp = torch.from_numpy(obs_tmp).float().to(device)\n",
        "\n",
        "        if game.done:\n",
        "            if game.winner == 0:\n",
        "                reward = 1\n",
        "                wins.append(1)\n",
        "            else:\n",
        "                wins.append(0)\n",
        "            obs_post = obs_tmp\n",
        "        else:\n",
        "            op_action = np.argmax(game.hand[game.player],axis=0)[0]\n",
        "            obs_post, _, done = game.step(op_action)\n",
        "            if game.done:\n",
        "                if game.winner == 0:\n",
        "                    reward = 1\n",
        "                    wins.append(1)\n",
        "                else:\n",
        "                    wins.append(0)\n",
        "            else:\n",
        "                reward = 0\n",
        "\n",
        "        rsum += reward\n",
        "\n",
        "        if isinstance(obs_post, np.ndarray):\n",
        "          obs_post = torch.from_numpy(obs_post).float()\n",
        "        elif torch.is_tensor(obs_post):\n",
        "          obs_post = obs_post.float()\n",
        "\n",
        "        buffer.append((obs, action, reward, obs_post, done))\n",
        "\n",
        "        if totalstep > initialize:\n",
        "          samples = buffer.sample(batchsize)\n",
        "          states = [torch.tensor(sample[0], dtype=torch.float32).clone().detach().cpu() for sample in samples]\n",
        "          states = torch.stack(states).to(device)\n",
        "\n",
        "          #print(states)\n",
        "          actions = [torch.tensor(sample[1], dtype=torch.long).clone().detach().cpu() for sample in samples]\n",
        "          actions = torch.stack(actions).to(device)\n",
        "\n",
        "          rewards = [torch.tensor(sample[2], dtype=torch.float32) for sample in samples]\n",
        "          rewards = torch.stack(rewards).to(device)\n",
        "\n",
        "          states_post = [torch.tensor(sample[3], dtype=torch.float32).clone().detach().cpu() for sample in samples]\n",
        "          states_post = torch.stack(states_post).to(device)\n",
        "\n",
        "          dones = [torch.tensor(sample[4], dtype=torch.float32) for sample in samples]\n",
        "\n",
        "          dones = torch.stack(dones).to(device)\n",
        "\n",
        "\n",
        "          train(states, actions, rewards, states_post, dones, actor, critic, actor_optimizer, critic_optimizer)\n",
        "\n",
        "        obs = obs_post\n",
        "\n",
        "    rrecord.append(rsum)\n",
        "    disp_number = 50\n",
        "    if ite % disp_number == 0:\n",
        "        if wins:\n",
        "            win_rate = int(np.mean(wins[-disp_number:])*100)\n",
        "        else:\n",
        "            win_rate = 0\n",
        "        print('iteration {} ave reward {}, win rate {}'.format(ite, np.mean(rrecord[-disp_number:]), win_rate))\n",
        "\n",
        "    if ite > 100:\n",
        "        ave100 = np.mean(wins[-100:])\n",
        "        if ave100 > 0.9:\n",
        "            torch.save(actor.state_dict(), 'models/actor_solved')\n",
        "            torch.save(critic.state_dict(), 'models/critic_solved')\n",
        "            print(\"Solved after %d episodes.\" % ite)\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMz9eeE-840L"
      },
      "outputs": [],
      "source": [
        "torch.save(actor.state_dict(), 'models/actor_model_'+str(ite)+'.pth' )\n",
        "torch.save(critic.state_dict(), 'models/critic_model_'+str(ite)+'.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "VEeodT033qmR",
        "outputId": "c8c31248-d8f9-4833-fff6-c03830294738"
      },
      "outputs": [],
      "source": [
        "# plot [episode, reward] history\n",
        "x = [i+1 for i in range(len(wins))]\n",
        "wr = [np.mean(wins[i-1000:i]) for i in range(len(wins))]\n",
        "plt.plot(x, wr)\n",
        "# plt.title('episode rewards')\n",
        "plt.xlabel('episodes')\n",
        "plt.ylabel('win rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUbJy5Hy3qmR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "rrecord = []\n",
        "totalstep = 0\n",
        "wins = []\n",
        "for ite in range(1000):\n",
        "    obs = torch.from_numpy(game.reset(display=0)).float().to(device)\n",
        "    done = False\n",
        "    rsum = 0\n",
        "\n",
        "    while not done:\n",
        "        totalstep += 1\n",
        "        if np.random.rand() < max(epsilon, 1-(1-epsilon)/episodes/0.6*ite):\n",
        "            action = random.choice(range(actsize))\n",
        "        else:\n",
        "            action_probs = actor(obs.to(device))\n",
        "            #print(action_probs)\n",
        "            #actionMask = np.array([(i % 6 in game.validCard[game.player]) and (i // 6 in game.validStone[game.player]) for i in range(actsize)])\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "        obs_tmp, _, done = game.step(action)\n",
        "        obs_tmp = torch.from_numpy(obs_tmp).float().to(device)\n",
        "\n",
        "        if game.done:\n",
        "            if game.winner == 0:\n",
        "                reward = 1\n",
        "                wins.append(1)\n",
        "            else:\n",
        "                wins.append(0)\n",
        "            obs_post = obs_tmp\n",
        "        else:\n",
        "            op_action = np.argmax(game.hand[game.player],axis=0)[0]\n",
        "            obs_post, _, done = game.step(op_action)\n",
        "            if game.done:\n",
        "                if game.winner == 0:\n",
        "                    reward = 1\n",
        "                    wins.append(1)\n",
        "                else:\n",
        "                    wins.append(0)\n",
        "            else:\n",
        "                reward = 0\n",
        "\n",
        "        rsum += reward\n",
        "\n",
        "        if isinstance(obs_post, np.ndarray):\n",
        "          obs_post = torch.from_numpy(obs_post).float()\n",
        "        elif torch.is_tensor(obs_post):\n",
        "          obs_post = obs_post.float()\n",
        "\n",
        "        \n",
        "        obs = obs_post\n",
        "\n",
        "print(np.mean(wins))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "conda_RL",
      "language": "python",
      "name": "conda_rl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
