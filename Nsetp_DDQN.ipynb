{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vcd9X9gTiZY"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKonQSRKTiZl"
   },
   "outputs": [],
   "source": [
    "def Qfunction(input_nodes, hidden_layer_size, output_nodes, opt):\n",
    "\tmodel = keras.models.Sequential()\n",
    "        \n",
    "\tmodel.add(keras.layers.Dense(input_nodes, activation='selu', kernel_initializer='lecun_normal', input_dim=input_nodes))\n",
    "\t\n",
    "\tmodel.add(keras.layers.Dense(input_nodes, activation='selu', kernel_initializer='lecun_normal'))\n",
    "\tfor ii in range(len(hidden_layer_size)):\n",
    "\t\tmodel.add(keras.layers.Dense(hidden_layer_size[ii], activation='selu', kernel_initializer='lecun_normal'))\n",
    "\t\n",
    "\tmodel.add(keras.layers.Dense(output_nodes, activation='selu', kernel_initializer='lecun_normal'))\n",
    "\tmodel.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "\treturn model\n",
    "\n",
    "def Qtest(input_nodes, hidden_layer_size, output_nodes, opt):\n",
    "\tmodel = keras.models.Sequential()\n",
    "        \n",
    "\tmodel.add(keras.layers.Dense(input_nodes, activation='selu', kernel_initializer='lecun_normal', input_dim=input_nodes))\n",
    "\t\n",
    "\tmodel.add(keras.layers.Dense(input_nodes, activation='selu', kernel_initializer='lecun_normal'))\n",
    "\tfor ii in range(len(hidden_layer_size)):\n",
    "\t\tmodel.add(keras.layers.Dense(hidden_layer_size[ii], activation='selu', kernel_initializer='lecun_normal'))\n",
    "\t\n",
    "\tmodel.add(keras.layers.Dense(output_nodes, activation='selu', kernel_initializer='lecun_normal'))\n",
    "\tmodel.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfiuGhwo3Jdg"
   },
   "outputs": [],
   "source": [
    "# Wrapper class for training Qfunction and updating weights (target network) \n",
    "\n",
    "class DQN(object):\n",
    "    \n",
    "    def __init__(self, obssize, actsize, hidden_dims, optimizer):\n",
    "        self.qfunction = Qfunction(obssize,hidden_dims,actsize,optimizer)\n",
    "        self.optimizer = optimizer\n",
    "        self.obssize = obssize\n",
    "        self.actsize = actsize\n",
    "\n",
    "    def _predict_q(self, states, actions):\n",
    "        Qvalues = self.compute_Qvalues(states)\n",
    "        Qpreds = tf.gather_nd(Qvalues, [[i, actions[i]] for i in range(len(actions))])\n",
    "        return Qpreds\n",
    "        \n",
    "\n",
    "    def _loss(self, Qpreds, targets):\n",
    "        return tf.math.reduce_mean(tf.square(Qpreds - targets))\n",
    "\n",
    "    \n",
    "    def compute_Qvalues(self, states):\n",
    "        inputs = tf.experimental.numpy.atleast_2d(states.astype('float32'))\n",
    "        return self.qfunction(inputs)\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def train(self, states, actions, targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            Qpreds = self._predict_q(states, actions)\n",
    "            loss = self._loss(Qpreds, targets)\n",
    "        variables = self.qfunction.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return loss\n",
    "\n",
    "    def update_weights(self, from_network):\n",
    "        from_var = from_network.qfunction.trainable_variables\n",
    "        to_var = self.qfunction.trainable_variables\n",
    "        \n",
    "        for v1, v2 in zip(from_var, to_var):\n",
    "            v2.assign(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiAwMOvRTiZn"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, maxlength):\n",
    "        self.buffer = deque()\n",
    "        self.number = 0\n",
    "        self.maxlength = maxlength\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        self.number += 1\n",
    "        if(self.number > self.maxlength):\n",
    "            self.pop()\n",
    "        \n",
    "    def pop(self):\n",
    "        while self.number > self.maxlength:\n",
    "            self.buffer.popleft()\n",
    "            self.number -= 1\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        inds = np.random.choice(len(self.buffer), batchsize, replace=False)\n",
    "        return [self.buffer[idx] for idx in inds]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from Agent import Agent\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "import Game2\n",
    "importlib.reload(Game2)\n",
    "from Game2 import Game\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "lr = 1e-5\n",
    "batchsize = 64\n",
    "maxlength = 100000 \n",
    "tau = 1000  \n",
    "initialize = 5000  \n",
    "epsilon = .1  \n",
    "gamma = .95 \n",
    "hidden_dims=[128, 256, 512, 512, 265, 64] \n",
    "nstep = 6\n",
    "episodes = 200000\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "game = Game(9)\n",
    "obssize = 120\n",
    "actsize = 54\n",
    "Qprincipal = DQN(obssize, actsize, hidden_dims, optimizer)\n",
    "Qtarget = DQN(obssize, actsize, hidden_dims, optimizer)\n",
    "buffer = ReplayBuffer(maxlength)\n",
    "Train_start = True\n",
    "\n",
    "rrecord = []\n",
    "totalstep = 0\n",
    "\n",
    "wins = []\n",
    "unif_agent = Agent()\n",
    "maxWinRate = 0\n",
    "baseline = 1\n",
    "testWinSet = []\n",
    "for ite in range(episodes):\n",
    "\n",
    "    obs = game.reset(display=0)\n",
    "    done = False\n",
    "    rsum = 0\n",
    "    gameStep = 0\n",
    "    stateDeque = []\n",
    "    actionDeque = []\n",
    "    rewardDeque = []\n",
    "    while not done:\n",
    "        totalstep += 1\n",
    "        gameStep += 1\n",
    "        if np.random.rand() < max(epsilon,1-(1-epsilon)*ite/episodes/0.3):\n",
    "            action = random.choice(range(actsize))\n",
    "        else:\n",
    "            Q = Qprincipal.compute_Qvalues(obs)\n",
    "            actionMask = np.array([(i%6 in game.validCard[game.player]) and (i//6 in game.validStone[game.player]) for i in range(actsize)])\n",
    "            action = np.argmax(Q*actionMask)\n",
    "        \n",
    "        obs_tmp, reward, done = game.step(action)\n",
    "        if game.done:\n",
    "            if game.winner == 0:\n",
    "                wins.append(1)\n",
    "            else:\n",
    "                wins.append(0)\n",
    "            obs_post = obs_tmp\n",
    "        else:\n",
    "            op_action = unif_agent.action(validCard=game.validCard[game.player], validStone=game.validStone[game.player])\n",
    "            obs_post, _, done = game.step(op_action)\n",
    "            if game.done:\n",
    "                if game.winner == 0:\n",
    "                    wins.append(1)\n",
    "                    reward += 50\n",
    "                else:\n",
    "                    wins.append(0)\n",
    "        if gameStep >= nstep:\n",
    "            if gameStep > nstep:\n",
    "                stateDeque.pop(0)\n",
    "                actionDeque.pop(0)\n",
    "                rewardDeque.pop(0)\n",
    "            stateDeque.append(obs)\n",
    "            actionDeque.append(action)\n",
    "            rewardDeque.append(reward)\n",
    "            if done:\n",
    "                for ii in range(nstep):\n",
    "                    buffer.append((stateDeque[ii], actionDeque[ii], rewardDeque, done, obs_post))\n",
    "                    rewardDeque.pop(0)\n",
    "                    rewardDeque.append(0)\n",
    "            else:\n",
    "                buffer.append((stateDeque[0], actionDeque[0], rewardDeque, done, obs_post))\n",
    "\n",
    "        else:\n",
    "            stateDeque.append(obs)\n",
    "            actionDeque.append(action)\n",
    "            rewardDeque.append(reward)\n",
    "\n",
    "        \n",
    "        rsum += reward\n",
    "        \n",
    "        obs = obs_post\n",
    "\n",
    "        if totalstep > initialize:\n",
    "            if hi:\n",
    "                print('start training')\n",
    "                hi = False\n",
    "            samples = buffer.sample(batchsize)\n",
    "            states = np.array([sample[0] for sample in samples])\n",
    "            actions = np.array([sample[1] for sample in samples])\n",
    "            rewards = np.array([sample[2] for sample in samples])\n",
    "            dones = np.array([sample[3] for sample in samples])\n",
    "            states_post = np.array([sample[4] for sample in samples])\n",
    "            G = np.zeros(batchsize)\n",
    "            for jj in range(nstep):\n",
    "                G += gamma**jj * rewards[:,jj]\n",
    "                \n",
    "            q_values = Qprincipal.compute_Qvalues(states_post)\n",
    "            actions_post = tf.argmax(q_values, axis=1)\n",
    "            q_values_next_state = Qtarget.compute_Qvalues(states_post) \n",
    "            double_q = tf.gather_nd(q_values_next_state, [[i, actions_post[i]] for i in range(len(actions_post))])\n",
    "            target = G + gamma**nstep * double_q * (1-dones)\n",
    "            Qprincipal.train(states, actions, target)\n",
    "\n",
    "            if totalstep % tau == 0:\n",
    "                Qtarget.update_weights(Qprincipal)\n",
    "    rrecord.append(rsum)\n",
    "    disp_number = 50\n",
    "    if ite % disp_number == 0:\n",
    "        print('iteration {},  win rate {}, reward {}'.format(ite, int(np.mean(wins[-disp_number:])*100), np.mean(rrecord[-disp_number:])))\n",
    "    if ite > 100:\n",
    "        ave100 = np.mean(wins[-100:])\n",
    "        if ave100 > 0.9:\n",
    "            Qprincipal.qfunction.save('models/model_solved')\n",
    "            print(\"Solved after %d episodes.\"%ite)\n",
    "            break\n",
    "        elif ite % 1000 == 0:\n",
    "            Qprincipal.qfunction.save('models/model_'+str(ite))\n",
    "            test_model = DQN(obssize, actsize, hidden_dims, optimizer)\n",
    "            test_model.qfunction.set_weights(Qprincipal.qfunction.get_weights())\n",
    "            testWins = 0\n",
    "            for jj in range(1000):\n",
    "                game.reset(display=0)\n",
    "                done = False\n",
    "                while not done:\n",
    "                    if game.player == 0:\n",
    "                        Q = test_model.compute_Qvalues(obs)\n",
    "                        actionMask = np.array([(i%6 in game.validCard[game.player]) and (i//6 in game.validStone[game.player]) for i in range(actsize)])\n",
    "                        action = np.argmax(Q*actionMask)\n",
    "                    else:\n",
    "                        action = unif_agent.action(validCard=game.validCard[game.player], validStone=game.validStone[game.player])\n",
    "                    obs, _, done = game.step(action)\n",
    "                if game.winner == 0:\n",
    "                    testWins += 0.1\n",
    "            print('test win rate (%): ', testWins)\n",
    "            testWinSet.append(testWins)\n",
    "Qprincipal.qfunction.save('models/model_'+str(ite+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i+1 for i in range(len(wins))]\n",
    "wr = [np.mean(wins[i-100:i]) for i in range(len(wins))]\n",
    "plt.plot(x, wr)\n",
    "plt.xlabel('episodes')\n",
    "plt.ylabel('win rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_RL",
   "language": "python",
   "name": "conda_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
